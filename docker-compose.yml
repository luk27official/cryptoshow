services:
  backend:
    build: ./backend
    volumes:
      - ./data:/app/data
      - ./cache/torch:/root/.cache/torch/hub/checkpoints
      - ./cache/cryptobench:/app/cryptobench
      - ./cache/cryptobench-small:/app/cryptobench-small
    ports:
      - "5000:5000"
    restart: unless-stopped
    depends_on:
      - redis
    environment:
      - CELERY_BROKER=redis://redis:6379/0
      - CELERY_BACKEND=redis://redis:6379/0
      - PYTHONPATH=/app
    working_dir: /app

  worker-gpu:
    build: 
      context: ./backend
    command: celery -A tasks.celery_app worker --loglevel=info
    volumes:
      - ./data:/app/data
      - ./cache/torch:/root/.cache/torch/hub/checkpoints
      - ./cache/cryptobench:/app/cryptobench
      - ./cache/cryptobench-small:/app/cryptobench-small
    depends_on:
      - redis
      - backend
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - CELERY_BROKER=redis://redis:6379/0
      - CELERY_BACKEND=redis://redis:6379/0
      - PYTHONPATH=/app
      - NVIDIA_DISABLE_REQUIRE=true
      # - TF_GPU_ALLOCATOR=cuda_malloc_async
      # - TF_FORCE_GPU_ALLOW_GROWTH=true
      # - TF_USE_LEGACY_KERAS=1
    working_dir: /app
    profiles:
      - gpu

  worker-cpu:
    build: 
      context: ./backend
    command: celery -A tasks.celery_app worker --loglevel=info
    volumes:
      - ./data:/app/data
      - ./cache/torch:/root/.cache/torch/hub/checkpoints
      - ./cache/cryptobench:/app/cryptobench
      - ./cache/cryptobench-small:/app/cryptobench-small
    depends_on:
      - redis
      - backend
    environment:
      - CELERY_BROKER=redis://redis:6379/0
      - CELERY_BACKEND=redis://redis:6379/0
      - PYTHONPATH=/app
    working_dir: /app
    profiles:
      - cpu

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"

  frontend:
    build: ./frontend
    depends_on:
      - backend
    ports:
      - "80:80"
    restart: unless-stopped
